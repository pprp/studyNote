# 深度学习阅读笔记

## 一、注意力机制

### 1.1 注意力机制原理和方法

来源：<https://blog.heuritech.com/2016/01/20/attention-mechanism/>

微信：[https://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&mid=2247485350&idx=1&sn=3e79cf2c009af1da369c8da16a9b53bb&chksm=96f375f2a184fce47f0e2459642df3ec6077b9073358dc39526b358953c4b2dd35dd41c57b67&mpshare=1&scene=23&srcid=12010TqQUvmoGFwxCOfhjkiE&sharer_sharetime=1575215773081&sharer_shareid=c16b66222c9e09eae50ae8f9081b0495#rd](#rd)

 评价：无意义

 



 

 

 

 

 

 

 

## 二、CNN模型

 

 

## 三、剪枝加速

 

 

## 四、语义分割

 

 

## 五、调参技巧

### 5.1 分类任务tricks

链接：

[https://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&mid=2247485783&idx=2&sn=10662ca037f4f2f11d9fc5854a778717&chksm=96f37b03a184f21532d223e28b4fec322a95669a7c1ea2f85f4da7f6435211b1c2cac6357a54&mpshare=1&scene=23&srcid=12017D48fUf2hqihtC3h0DFq&sharer_sharetime=1575215733637&sharer_shareid=c16b66222c9e09eae50ae8f9081b0495#rd](#rd)

code：无

评价：待尝试

#### (1) Warmup

学习率预热就是在刚开始训练的时候先使用一个较小的学习率，训练一些epoches或iterations，等模型稳定时再修改为预先设置的学习率进行训练。论文[1]中使用一个110层的ResNet在cifar10上训练时，先用0.01的学习率训练直到训练误差低于80%(大概训练了400个iterations)，然后使用0.1的学习率进行训练。

上述的方法是constant warmup，18年Facebook又针对上面的warmup进行了改进[3]，因为从一个很小的学习率一下变为比较大的学习率可能会导致训练误差突然增大。论文[3]提出了gradual warmup来解决这个问题，即从最开始的小学习率开始，每个iteration增大一点，直到最初设置的比较大的学习率。

```python
class GradualWarmupScheduler(_LRScheduler):
    """
    Args:
        mutliplier : target learnnig rate = base lr *multiplier
        total epoch : target learning rate is reached at total epoch gradually
        after scheduler: after target_epoch , use this schedueler(ReduceLROnPlateau)
    """

    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):
        self.multiplier = multiplier
        if self.multiplier <= 1.:
            raise ValueError('mutliplier should be greater than 1')
        self.total_epoch = total_epoch
        self.after_scheduler = after_scheduler
        self.finished = False
        super().__init__(optimizer)

    def get_lr(self):
        if self.last_epoch > self.total_epoch:
            if self.after_scheduler:
                if not self.finished:
                    self.after_scheduler.base_lrs = [
                        base_lr * self.multiplier for base_lr in self.base_lrs]
                    self.finished = True
                return self.after_scheduler.get_lr()
            return [base_lr * self.multiplier for base_lr in self.base_lrs]
        return [base_lr * ((self.multiplier-1.0)*self.last_epoch / self.total_epoch+1.) for base_lr in self.base_lrs]

    def step(self, epoch=None):
        if self.finished and self.after_scheduler:
            return self.after_scheduler.step(epoch)
        else:
            return super(GradualWarmupScheduler, self).step(epoch)
```

####  (2) label smooth

在分类问题中，我们的最后一层一般是全连接层，然后对应标签的one-hot编码，即把对应类别的值编码为1，其他为0。这种编码方式和通过降低交叉熵损失来调整参数的方式结合起来，会有一些问题。这种方式会鼓励模型对不同类别的输出分数差异非常大，或者说，模型过分相信它的判断。但是，对于一个由多人标注的数据集，不同人标注的准则可能不同，每个人的标注也可能会有一些错误。模型对标签的过分相信会导致过拟合。

标签平滑(Label-smoothing regularization,LSR)是应对该问题的有效方法之一，它的具体思想是降低我们对于标签的信任，例如我们可以将损失的目标值从1稍微降到0.9，或者将从0稍微升到0.1。标签平滑最早在inception-v2[4]中被提出，它将真实的概率改造为：

![1575255859418](1575255859418.png)

其中，ε是一个小的常数，K是类别的数目，y是图片的真正的标签，i代表第i个类别，q_i是图片为第i类的概率。

总的来说，LSR是一种通过在标签y中加入噪声，实现对模型约束，降低模型过拟合程度的一种正则化方法。

LSR:

```python
import torch
import torch.nn as nn
 
class NMTCritierion(nn.Module):
    """
    TODO:
    1. Add label smoothing
    """
    def __init__(self, label_smoothing=0.0):
        super(NMTCritierion, self).__init__()
        self.label_smoothing = label_smoothing
        self.LogSoftmax = nn.LogSoftmax()
 
        if label_smoothing > 0:
            self.criterion = nn.KLDivLoss(size_average=False)
        else:
            self.criterion = nn.NLLLoss(size_average=False, ignore_index=100000)
        self.confidence = 1.0 - label_smoothing
 
    def _smooth_label(self, num_tokens):
        # When label smoothing is turned on,
        # KL-divergence between q_{smoothed ground truth prob.}(w)
        # and p_{prob. computed by model}(w) is minimized.
        # If label smoothing value is set to zero, the loss
        # is equivalent to NLLLoss or CrossEntropyLoss.
        # All non-true labels are uniformly set to low-confidence.
        one_hot = torch.randn(1, num_tokens)
        one_hot.fill_(self.label_smoothing / (num_tokens - 1))
        return one_hot
 
    def _bottle(self, v):
        return v.view(-1, v.size(2))
 
    def forward(self, dec_outs, labels):
        scores = self.LogSoftmax(dec_outs)
        num_tokens = scores.size(-1)
 
        # conduct label_smoothing module
        gtruth = labels.view(-1)
        if self.confidence < 1:
            tdata = gtruth.detach()
            one_hot = self._smooth_label(num_tokens)  # Do label smoothing, shape is [M]
            if labels.is_cuda:
                one_hot = one_hot.cuda()
            tmp_ = one_hot.repeat(gtruth.size(0), 1)  # [N, M]
            tmp_.scatter_(1, tdata.unsqueeze(1), self.confidence)  # after tdata.unsqueeze(1) , tdata shape is [N,1]
            gtruth = tmp_.detach()
        loss = self.criterion(scores, gtruth)
        return loss
```

LSR2:

```python
import torch
import torch.nn as nn
class LabelSmoothSoftmaxCE(nn.Module):
    def __init__(self,
                 lb_pos=0.9,
                 lb_neg=0.005,
                 reduction='mean',
                 lb_ignore=255,
                 ):
        super(LabelSmoothSoftmaxCE, self).__init__()
        self.lb_pos = lb_pos
        self.lb_neg = lb_neg
        self.reduction = reduction
        self.lb_ignore = lb_ignore
        self.log_softmax = nn.LogSoftmax(1)

    def forward(self, logits, label):
        logs = self.log_softmax(logits)
        ignore = label.data.cpu() == self.lb_ignore
        n_valid = (ignore == 0).sum()
        label[ignore] = 0
        lb_one_hot = logits.data.clone().zero_().scatter_(1, label.unsqueeze(1), 1)
        label = self.lb_pos * lb_one_hot + self.lb_neg * (1-lb_one_hot)
        ignore = ignore.nonzero()
        _, M = ignore.size()
        a, *b = ignore.chunk(M, dim=1)
        label[[a, torch.arange(label.size(1)), *b]] = 0

        if self.reduction == 'mean':
            loss = -torch.sum(torch.sum(logs*label, dim=1)) / n_valid
        elif self.reduction == 'none':
            loss = -torch.sum(logs*label, dim=1)
        return loss


if __name__ == '__main__':
    torch.manual_seed(15)
    criterion = LabelSmoothSoftmaxCE(lb_pos=0.9, lb_neg=5e-3)
    
    out = torch.randn(10, 5).cuda()
    lbs = torch.randint(5, (10,)).cuda()
    print('out:', out)
    print('lbs:', lbs)

    import torch.nn.functional as F
    
    loss = criterion(out, lbs)
    print('loss:', loss)
```

LSR3:

```python
class LSR(nn.Module):
    def __init__(self, e=0.1, reduction='mean'):
        super().__init__()
        self.log_softmax = nn.LogSoftmax(dim=1)
        self.e = e
        self.reduction=reduction
    def _one_hot(self, labels, classes, value=1):
        """
        convert labels to one hot vectors
        args:
            labels: torch tensor [label1, label2...]
            classes: int, num of classes
            value: label value in one hot value, defalt to 1
        return:
            return one hot format labels in shape[bs, classes]
        """
        one_hot = torch.zeros(labels.size()[0], classes)
        labels = labels.view(labels.size()[0],-1)
        value_added = torch.Tensor(labels.size()[0], 1).fill_(value)
        
        value_added = value_added.to(labels.device)
        one_hot = one_hot.to(labels.device)

        one_hot.scatter_add_(1, labels, value_added)
        return one_hot
    def _smooth_label(self, target, length, smooth_factor):
        """
        args:
            targets: formate [label1, label2, label_batch size]
            length: length of one-hot format (num of classes)
            smooth facter: smooth factor
        """
        one_hot = self._one_hot(target, length, value=1-smooth_factor)
        one_hot += smooth_factor / length
        return one_hot.to(target.device)

```

#### (3) Linear scaling learning rate

Linear scaling learning rate是在论文[3]中针对比较大的batch size而提出的一种方法。

在凸优化问题中，随着批量的增加，收敛速度会降低，神经网络也有类似的实证结果。随着batch size的增大，处理相同数据量的速度会越来越快，但是达到相同精度所需要的epoch数量越来越多。也就是说，使用相同的epoch时，大batch size训练的模型与小batch size训练的模型相比，验证准确率会减小。

上面提到的gradual warmup是解决此问题的方法之一。另外，linear scaling learning rate也是一种有效的方法。在mini-batch SGD训练时，梯度下降的值是随机的，因为每一个batch的数据是随机选择的。增大batch size不会改变梯度的期望，但是会降低它的方差。也就是说，大batch size会降低梯度中的噪声，所以我们可以增大学习率来加快收敛。

具体做法很简单，比如ResNet原论文[1]中，batch size为256时选择的学习率是0.1，当我们把batch size变为一个较大的数b时，学习率应该变为 0.1 × b/256

#### (4) Random image cropping and patching

Random image cropping and patching (RICAP)[7]方法随机裁剪四个图片的中部分，然后把它们拼接为一个图片，同时混合这四个图片的标签。

![1575256732415](1575256732415.png)

如下图所示，Ix, Iy是原始图片的宽和高。w和h称为boundary position，它决定了四个裁剪得到的小图片的尺寸。w和h从beta分布Beta(β, β)中随机生成，β也是RICAP的超参数。最终拼接的图片尺寸和原图片尺寸保持一致。

```python
beta = 0.3

for (images, targets) in data_loader:
    # get image size
    I_x, I_y = images.size()[2:]
    # draw a boundary position w,h
    # draw a boundry position (w, h)
    w = int(np.round(I_x * np.random.beta(beta, beta)))
    h = int(np.round(I_y * np.random.beta(beta, beta)))
    w_ = [w, I_x - w, w, I_x - w]
    h_ = [h, h, I_y - h, I_y - h]
    # select and crop four images
    cropped_images = {}
    c_ = {}
    W_ = {}
    for k in range(4):
        index = torch.randperm(images.size(0))
        x_k = np.random.randint(0, I_x - w_[k] + 1)
        y_k = np.random.randint(0, I_y - h_[k] + 1)
        cropped_images[k] = images[index][:, :,
                                          x_k:x_k + w_[k], y_k:y_k + h_[k]]
        c_[k] = target[index].cuda()
        W_[k] = w_[k] * h_[k] / (I_x * I_y)
        # patch cropped images
        patched_images = torch.cat(
            (torch.cat((cropped_images[0], cropped_images[1]), 2), 
            torch.cat((cropped_images[2], cropped_images[3]), 2)), 3)
        patched_images = patched_images.cuda()
        # get output
        output = model(patched_images)
        # calculate loss and accuracy
        loss = sum([W_[k] * criterion(output, c_[k]) for k in range(4)])
        acc = sum([W_[k] * accuracy(output, c_[k])[0] for k in range(4)])

```

#### (5) 知识蒸馏

提高几乎所有机器学习算法性能的一种非常简单的方法是在相同的数据上训练许多不同的模型，然后对它们的预测进行平均。但是使用所有的模型集成进行预测是比较麻烦的，并且可能计算量太大而无法部署到大量用户。Knowledge Distillation(知识蒸馏)[8]方法就是应对这种问题的有效方法之一。

在知识蒸馏方法中，我们使用一个教师模型来帮助当前的模型（学生模型）训练。教师模型是一个较高准确率的预训练模型，因此学生模型可以在保持模型复杂度不变的情况下提升准确率。比如，可以使用ResNet-152作为教师模型来帮助学生模型ResNet-50训练。在训练过程中，我们会加一个蒸馏损失来惩罚学生模型和教师模型的输出之间的差异。

给定输入，假定p是真正的概率分布，z和r分别是学生模型和教师模型最后一个全连接层的输出。之前我们会用交叉熵损失l(p,softmax(z))来度量p和z之间的差异，这里的蒸馏损失同样用交叉熵。所以，使用知识蒸馏方法总的损失函数是

![1575259852786](1575259852786.png)

上式中，第一项还是原来的损失函数，第二项是添加的用来惩罚学生模型和教师模型输出差异的蒸馏损失。其中，T是一个温度超参数，用来使softmax的输出更加平滑的。实验证明，用ResNet-152作为教师模型来训练ResNet-50，可以提高后者的准确率。

#### (6) Cutoff

Cutout[9]是一种新的正则化方法。原理是在训练时随机把图片的一部分减掉，这样能提高模型的鲁棒性。它的来源是计算机视觉任务中经常遇到的物体遮挡问题。通过cutout生成一些类似被遮挡的物体，不仅可以让模型在遇到遮挡问题时表现更好，还能让模型在做决定时更多地考虑环境(context)。

```python
import torch 
import numpy as np 
class Cutout(object): 
    """Randomly mask out one or more patches from an image.
    Args:n_holes (int): 
        Number of patches to cut out of each image. 
        length (int): The length (in pixels) of each square patch. 
    """ 
    def __init__(self, n_holes, length): 
        self.n_holes = n_holes self.length = length 
        
    def __call__(self, img): 
        """ Args:img (Tensor): 
                Tensor image of size (C, H, W). 
            Returns: 
                Tensor: Image with n_holes of dimension length x length cut out of it. 
        """ 
        h = img.size(1) w = img.size(2) 
        mask = np.ones((h, w), np.float32)
        for n in range(self.n_holes):
            y = np.random.randint(h) 
            x = np.random.randint(w) 
            y1 = np.clip(y - self.length // 2, 0, h) 
            y2 = np.clip(y + self.length // 2, 0, h) 
            x1 = np.clip(x - self.length // 2, 0, w) 
            x2 = np.clip(x + self.length // 2, 0, w) 
            mask[y1: y2, x1: x2] = 0. 

        mask = torch.from_numpy(mask) 
        mask = mask.expand_as(img) 
        img = img * mask 
        return img
```

#### (7) Random erasing 

Random erasing[6]其实和cutout非常类似，也是一种模拟物体遮挡情况的数据增强方法。区别在于，cutout是把图片中随机抽中的矩形区域的像素值置为0，相当于裁剪掉，random erasing是用随机数或者数据集中像素的平均值替换原来的像素值。而且，cutout每次裁剪掉的区域大小是固定的，Random erasing替换掉的区域大小是随机的。

#### (8) Cosine learning rate decay

在warmup之后的训练过程中，学习率不断衰减是一个提高精度的好方法。其中有step decay和cosine decay等，前者是随着epoch增大学习率不断减去一个小的数，后者是让学习率随着训练过程曲线下降。

对于cosine decay，假设总共有T个batch（不考虑warmup阶段），在第t个batch时，学习率η_t为：
  ![1575260997571](1575260997571.png)

这里，η代表初始设置的学习率。这种学习率递减的方式称之为cosine decay。

下面是带有warmup的学习率衰减的可视化图[4]。其中，图(a)是学习率随epoch增大而下降的图，可以看出cosine decay比step decay更加平滑一点。图(b)是准确率随epoch的变化图，两者最终的准确率没有太大差别，不过cosine decay的学习过程更加平滑。

![1575261028733](1575261028733.png)

 

#### (9) Mixup training

Mixup[10]是一种新的数据增强的方法。Mixup training，就是每次取出2张图片，然后将它们线性组合，得到新的图片，以此来作为新的训练样本，进行网络的训练，如下公式，其中x代表图像数据，y代表标签，则得到的新的$\hat{x}, \hat{y}$。

$$
\hat{x} = \lambda x_i+(1-\lambda)x_j, \\
\hat{y} = \lambda y_i+(1-\lambda)y_j
$$

其中，λ是从Beta(α, α)随机采样的数，在[0,1]之间。在训练过程中，仅使用(xhat, yhat)。

Mixup方法主要增强了训练样本之间的线性表达，增强网络的泛化能力，不过mixup方法需要较长的时间才能收敛得比较好。

Mixup代码如下：

```python
## mixup
for (images, labels) in train_loader:
    l = np.random.beta(mixup_alpha, mixup_alpha) 
    index = torch.randperm(images.size(0)) 
    images_a, images_b = images, images[index] 
    labels_a, labels_b = labels, labels[index] 
    mixed_images = l * images_a + (1 - l) * images_b 
    outputs = model(mixed_images) 
    loss = l * criterion(outputs, labels_a) + (1 - l) * criterion(outputs, labels_b) 
    acc = l * accuracy(outputs, labels_a)[0] + (1 - l) * accuracy(outputs, labels_b)[0]

```

#### (10) adabound

AdaBound是最近一篇论文[5]中提到的，按照作者的说法，AdaBound会让你的训练过程像adam一样快，并且像SGD一样好。

如下图所示，使用AdaBound会收敛速度更快，过程更平滑，结果更好。

![1575261394997](1575261394997.png)

另外，这种方法相对于SGD对超参数的变化不是那么敏感，也就是说鲁棒性更好。但是，针对不同的问题还是需要调节超参数的，只是所用的时间可能变少了。

![1575261432431](1575261432431.png)

当然，AdaBound还没有经过普遍的检验，也有可能只是对于某些问题效果好。

使用方法如下： 安装AdaBound

```
pip install adabound
```

使用AdaBound(和其他PyTorch optimizers用法一致)

```
optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)
```

#### (11) Auto Augment

数据增强在图像分类问题上有很重要的作用，但是增强的方法有很多，并非一股脑地用上所有的方法就是最好的。那么，如何选择最佳的数据增强方法呢？ AutoAugment[11]就是一种搜索适合当前问题的数据增强方法的方法。该方法创建一个数据增强策略的搜索空间，利用搜索算法选取适合特定数据集的数据增强策略。此外，从一个数据集中学到的策略能够很好地迁移到其它相似的数据集上。

AutoAugment在cifar10上的表现如下表，达到了98.52%的准确率。

![1575261560910](1575261560910.png)

#### (12) other

**常用的正则化方法为**

- Dropout
- L1/L2正则
- Batch Normalization
- Early stopping
- Random cropping
- Mirroring
- Rotation
- Color shifting
- PCA color augmentation
- ...

**其他**

- Xavier init[12]
- ...

### 5.2 目标检测tricks

arxiv: Bag of Freebies for Training Object Detection Neural Networks

1.视觉一致的Image Mixup（Visually Coherent Image Mixup for Object De- tection）

2.分类头标签平滑（Classification Head Label Smoothing）

3.数据预处理（Data Pre-processing）

主要是随机几何变换和颜色扰动。

4.训练调度程序改造（Training Scheduler Revamping）

改进学习率的衰减方法，使用cosine schedule 代替step schedule取得了更好的结果，如下图：

![1575275172068](1575275172068.png)

5.同步批归一化（Synchronized Batch Normalization）

方便多GPU训练。

6.随机形状训练（Random shapes training for single-stage object detection networks）











## 六、人脸识别

综述链接：<http://www.elecfans.com/d/709424.html>

 

 

 

## 七、模型设计

### 7.1 ACNet

From:[https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247493985&idx=4&sn=c98ecd11cb3ed6d6f0bf827da9187708&chksm=f9a19beeced612f8b0f6fe6d78bc19d9805136cd60d376ec462dadc661fe091319be147f4698&mpshare=1&scene=23&srcid=&sharer_sharetime=1575249200518&sharer_shareid=41b434eda6085c4278d4ad01a4465363#rd](#rd)

Code:https://github.com/ShawnDing1994/ACN

Conclusion: 不太好实现,精度提升0-1%

 

**3\*3卷积+1\*3卷积+3\*1卷积=白给的精度提升**

我们提出了非对称卷积块(ACB)作为CNN的构造块，它使用一维非对称卷积核来增强方形卷积核，我们用ACBs代替标准的方形卷积核来构造一个非堆成卷积网络ACNet，该网络可以训练到更高的精度。训练后，我们等价地将ACNet转换为相同的原始架构，因此将不需要额外的计算。实验证明，ACNet可以CIFAR和ImageNet上显著提高各种经典模型的性能。

 

 ## 八、 目标检测

### 8.1 小目标检测

source: <http://bbs.cvmart.net/topics/1245?from=groupmessage>

在深度学习目标检测中，特别是人脸检测中，小目标、小人脸的检测由于**分辨率低，图片模糊，信息少，噪音多，**所以一直是一个实际且常见的困难问题。不过在这几年的发展中，也涌现了一些提高小目标检测性能的解决手段，本文对这些手段做一个分析、整理和总结。

#### 1. **传统的图像金字塔和多尺度滑动窗口检测**

最开始在深度学习方法流行之前，对于不同尺度的目标，大家普遍使用将原图build出**不同分辨率的图像金字塔**，再对每层金字塔用固定输入分辨率的分类器在该层滑动来检测目标，以求在金字塔底部检测出小目标；或者只用一个原图，在原图上，用**不同分辨率的分类器**来检测目标，以求在比较小的窗口分类器中检测到小目标。

在著名的人脸检测器 [MTCNN] (Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks)中，就使用了图像金字塔的方法来检测不同分辨率的人脸目标。

![1575275673819](1575275673819.png)

后面有人借鉴它的思想搞出了特征金字塔网络FPN，它在不同层取特征进行融合，只需要一次前向计算，不需要缩放图片，也在小目标检测中得到了应用

#### 2. **Data Augmentation**

小目标检测的性能同样也可以通过增加训练集中小目标样本的种类和数量来提升。在《深度学习中不平衡样本的处理》一文中已经介绍了许多数据增强的方案，这些方案虽然主要是解决不同类别样本之间数量不均衡的问题的，但是有时候小目标检测之难其中也有数据集中小样本相对于大样本来说数量很少的因素，所以其中很多方案都可以用在小样本数据的增强上。

在19年的论文Augmentation for small object detection中，也提出了两个简单粗暴的方法：

- **针对COCO数据集中包含小目标的图片数量少的问题，使用过采样OverSampling策略；**
- **针对同一张图片里面包含小目标数量少的问题，在图片内用分割的Mask抠出小目标图片再使用复制粘贴的方法（当然，也加上了一些旋转和缩放，另外要注意不要遮挡到别的目标）。**在同一张图中有更多的小目标，在Anchor策略的方法中就会匹配出更多的正样本。

#### 3. **特征融合的FPN**

不同阶段的特征图对应的感受野不同，它们表达的信息抽象程度也不一样。浅层的特征图感受野小，比较适合检测小目标（要检测大目标，则其只“看”到了大目标的一部分，有效信息不够）；深层的特征图感受野大，适合检测大目标（要检测小目标，则其”看“到了太多的背景噪音，冗余噪音太多）。所以，有人就提出了将不同阶段的特征图，都融合起来，来提升目标检测的性能，这就是特征金字塔网络[FPN] (Feature Pyramid Networks for Object Detection)。

![1575275918058](1575275918058.png)

在人脸领域，基本上性能好一点的方法都是用了FPN的思想，其中比较有代表性的有RetinaFace: Single-stage Dense Face Localisation in the Wild

![1575275956193](1575275956193.png)

既然可以在不同分辨率特征图做融合来提升特征的丰富度和信息含量来检测不同大小的目标，那么自然也有人会进一步地猜想，如果只用高分辨率的特征图（浅层特征）去检测小脸；用中间分辨率的特征图（中层特征）去检测大脸；最后用地分辨率的特征图（深层特征）去检测小脸。比如人脸检测中的SSH。

![1575276040450](1575276040450.png)

#### 4. **合适的训练方法SNIP,SNIPER,SAN**

机器学习里面有个重要的观点，**模型预训练的分布要尽可能地接近测试输入的分布。**所以，在大分辨率（比如常见的224 x 224）下训练出来的模型，不适合检测本身是小分辨率再经放大送入模型的图片。如果是小分辨率的图片做输入，应该在小分辨率的图片上训练模型；再不行，应该用大分辨率的图片训练的模型上用小分辨率的图片来微调fine-tune；最差的就是直接用大分辨率的图片来预测小分辨率的图（通过上采样放大）。但是这是在理想的情况下的（训练样本数量、丰富程度都一样的前提下，但实际上，很多数据集都是小样本严重缺乏的），所以**放大输入图像+使用高分率图像预训练再在小图上微调，在实践中要优于专门针对小目标训练一个分类器。**

![1575276206997](1575276206997.png)

在下图中示意的是SNIP训练方法，**训练时只训练合适尺寸的目标样本，只有真值的尺度和Anchor的尺度接近时来用来训练检测器，太小太大的都不要，预测时输入图像多尺度，总有一个尺寸的Anchor是合适的，选择那个最合适的尺度来预测。**对 [R-FCN] (R-FCN: Object Detection via Region-based Fully Convolutional Networks)提出的改进主要有两个地方，一是多尺寸图像输入，针对不同大小的输入，在经过RPN网络时需要判断valid GT和invalid GT，以及valid anchor和invalid anchor，通过这一分类，使得得到的预选框更加的准确；二是在RCN阶段，根据预选框的大小，只选取在一定范围内的预选框，最后使用NMS来得到最终结果。

![1575276304321](1575276304321.png)

SNIPER是SNIP的实用升级版本，这里不做详细介绍了。

#### 6. **更稠密的Anchor采样和匹配策略S3FD,FaceBoxes**

在前面Data Augmentation部分已经讲了，复制小目标到一张图的多个地方可以增加小目标匹配的Anchor框的个数，增加小目标的训练权重，减少网络对大目标的bias。同样，反过来想，如果在数据集已经确定的情况下，我们也可以增加负责小目标的Anchor的设置策略来让训练时对小目标的学习更加充分。例如人脸检测中的 [FaceBoxes] (FaceBoxes: A CPU Real-time Face Detector with High Accuracy)其中一个Contribution就是Anchor densification strategy，Inception3的anchors有三个scales(32,64,128)，而32 scales是稀疏的，所以需要密集化4倍，而64 scales则需要密集化2倍。在S3FD人脸检测方法中，则用了Equal-proportion interval principle来保证不同大小的Anchor在图中的密度大致相等，这样大脸和小脸匹配到的Anchor的数量也大致相等了。

另外，对小目标的Anchor使用比较宽松的匹配策略（比如IoU > 0.4）也是一个比较常用的手段。

#### 7. **先生成放大特征再检测的GAN**

Perceptual GAN使用了GAN对小目标生成一个和大目标很相似的Super-resolved Feature（如下图所示），然后把这个Super-resolved Feature叠加在原来的小目标的特征图（如下下图所示）上，以此增强对小目标特征表达来提升小目标（在论文中是指交通灯）的检测性能。

![1575276492692](1575276492692.png)

#### 8. **利用Context信息的Relation Network和PyramidBox**

小目标，特别是像人脸这样的目标，不会单独地出现在图片中（想想单独一个脸出现在图片中，而没有头、肩膀和身体也是很恐怖的）。像[PyramidBox] (PyramidBox: A Context-assisted Single Shot Face Detector)方法，加上一些头、肩膀这样的上下文Context信息，那么目标就相当于变大了一些，上下文信息加上检测也就更容易了。

![1575276582697](1575276582697.png)

这里顺便再提一下通用目标检测中另外一种加入Context信息的思路，[Relation Networks] (Relation Networks for Object Detection)虽然主要是解决提升识别性能和过滤重复检测而不是专门针对小目标检测的，但是也和上面的PyramidBox思想很像的，都是利用上下文信息来提升检测性能，可以归类为Context一类。

![1575276615679](1575276615679.png)

### 8.2 目标检测中的不平衡问题

作者将不平衡问题分成四种类型，如下表：

![1575277997572](1575277997572.png)

1. 类别不平衡：前景和背景不平衡、前景中不同类别输入包围框的个数不平衡；
2. 尺度不平衡：输入图像和包围框的尺度不平衡，不同特征层对最终结果贡献不平衡；
3. 空间不平衡：不同样本对回归损失的贡献不平衡、正样本IoU分布不平衡、目标在图像中的位置不平衡；
4. 目标函数不平衡：不同任务（比如回归和分类）对全局损失的贡献不平衡。

主流目标检测算法的训练大致流程，与四种不平衡问题的示例：

![1575278091060](1575278091060.png)

作者将目前上述不平衡问题及相应目前学术界提出的解决方案，融合进了下面这张超有信息量的图（请点击查看大图）：



![img](BtyDbxs4tn.png)

作者又从方法的角度总结了这些解决不平衡问题的目标检测算法:

![file](OOUDV3pEZJ.png)

### 8.3 轻量级设计思路

#### 1. ThunderNet

国防科大+旷视：个能够在移动端ARM芯片实时运行的两阶段通用目标检测算法ThunderNet

source: https://arxiv.org/pdf/1903.11752v1.pdf

微信：<https://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&mid=2247486286&idx=1&sn=506920e5678dc20a95a515f8e2a2520c&chksm=96f3791aa184f00ce41d4e850fbe17acbb8ae3d627fbfba9a71abc01e48c086325a961224e3c&mpshare=1&scene=23&srcid=1201yRG2VTens1eJHA8WtuPZ&sharer_sharetime=1575215660641&sharer_shareid=c16b66222c9e09eae50ae8f9081b0495#rd>

![1575288072004](1575288072004.png)


![1575288098809](1575288098809.png)

2. 压缩RPN网络部分，降低候选目标区域生成的时间。

3. R-CNN子网络使用1024维的全连接层，提高速度。

**改进特征表示的鉴别性**

1. 设计了上下文增强模块CEM结构，它可以有效结合三个尺度的特征图，编码更多的上下文信息，增大感受野，生成更具鉴别性的特征。

    ![1575288153282](1575288153282.png)

    2. 设计了空间注意力模块SAM结构，它可以利用RPN中学习到的信息，从上下文增强模块提精特征图的特征分布。

        ![1575288180221](1575288180221.png)

        

#### 2. YOWO

arxiv:https://arxiv.org/pdf/1911.06644.pdf

评价：还未开源，行为识别

微信：<https://mp.weixin.qq.com/s?__biz=MzU4NTY4Mzg1Mw==&mid=2247486135&idx=3&sn=cdf76c79e39f386bd07467c47e6af103&chksm=fd878c55caf005436f9d50c8fa1d4b812417672dc47cbf1e6fd8ebd0bf28a3a0fb9bb5f0f6bf&mpshare=1&scene=23&srcid=&sharer_sharetime=1575215369958&sharer_shareid=c16b66222c9e09eae50ae8f9081b0495#rd>

![1575288609642](1575288609642.png)

使用3D CNN对视频片段提取时空信息，使用2D CNN对关键帧提取空间信息，对得到的两部分特征进行注意力机制和通道融合（CFAM）的特征聚合，后面的过程与YOLO一样，卷积后进行分类和包围框回归。

![1575288630370](1575288630370.png)



### 8.4 IoU

source:

<https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247492356&idx=1&sn=61d27d5e91b05e52dcd9ef02775e8867&chksm=ec1c0efddb6b87eb44bda5baf40a01a9bb01ecb6186ca3a34df83691e0b1c831027bdecba2a3&mpshare=1&scene=23&srcid=&sharer_sharetime=1575214780454&sharer_shareid=c16b66222c9e09eae50ae8f9081b0495#rd>

arxiv:https://arxiv.org/pdf/1911.08287.pdf

code: https://github.com/Zzh-tju/DIoU-darknet : c语言版



1. IoU：从IoU误差的曲线我们可以发现，anchor越靠近边缘，误差越大，那些与目标框没有重叠的anchor基本无法回归。
2. GIoU：从GIoU误差的曲线我们可以发现，对于一些没有重叠的anchor，GIoU的表现要比IoU更好。但是由于GIoU仍然严重的依赖IoU，因此在两个垂直方向，误差很大，基本很难收敛，这就是GIoU不稳定的原因。
3. DIoU：从DIoU误差的曲线我们可以发现，对于不同距离，方向，面积和比例的anchor，DIoU都能做到较好的回归。

![1575289542742](1575289542742.png)

**基于IoU和GIoU存在的问题，作者提出了两个问题：**

第一：直接最小化anchor框与目标框之间的归一化距离是否可行，以达到更快的收敛速度。

第二：如何使回归在与目标框有重叠甚至包含时更准确、更快。

**作者为了回答第一个问题：提出了Distance-IoU Loss。**

![1575289591878](1575289591878.png)

DIoU的优点如下：

1. 与GIoU loss类似，DIoU loss在与目标框不重叠时，仍然可以为边界框提供移动方向。
2. DIoU loss可以直接最小化两个目标框的距离，因此比GIoU loss收敛快得多。
3. 对于包含两个框在水平方向和垂直方向上这种情况，DIoU损失可以使回归非常快，而GIoU损失几乎退化为IoU损失。
4. DIoU还可以替换普通的IoU评价策略，应用于NMS中，使得NMS得到的结果更加合理和有效。

**作者为了回答第二个问题：提出了Complete-IoU Loss。**

作者认为一个好的目标框回归损失应该考虑三个重要的几何因素：重叠面积、中心点距离、长宽比。

GIoU：为了归一化坐标尺度，利用IoU，并初步解决IoU为零的情况。

DIoU：DIoU损失同时考虑了边界框的重叠面积和中心点距离。

然而，anchor框和目标框之间的长宽比的一致性也是极其重要的。基于此，作者提出了Complete-IoU Loss。

![1575289649678](1575289649678.png)

### 8.5 比赛经验

方案1：Faster R-CNN

> 预处理： 归一化， flip , crop
>
> 特征提取：FPN
>
> loss: Focal Loss
>
> soft nms
>
> learning rate: 余弦退火学习率

方案2：FPN+RFCN+DCN/Cascade DCN

> focal loss
>
> FPN

方案3：FPN, cascade

> backbone: resnet152+se
>
> 知识蒸馏
>
> 下采样减缓

方案4：yolo

> yolov3+attention+多尺度
>
> CBAM
>
> Angular softmax



## 九、姿态估计

### 9.1 轻量级

ariv:https://arxiv.org/pdf/1911.10346v1.pdf

在姿态估计算法中，微软开发的 SimpleBaseline 是精度高而又轻量级的典范，昨天一篇论文Simple and Lightweight Human Pose Estimation，在该架构基础上做了少许改进，取得了更快的速度和更小的模型Size。

![1575289751618](1575289751618.png)

![1575289775925](1575289775925.png)

另外，作者提出了一种在推断阶段对heatmap进行*Soft-Argmax*以获得更精确位置信息的方法*B-Soft-Argmax。*

## 十、图像检索

### 10.1 图像检索问题

code:<https://github.com/leeesangwon/PyTorch-Image-Retrieval>





## 十一、目标跟踪

综合awesome list: <http://bbs.cvmart.net/articles/265/zi-yuan-duo-mu-biao-zhui-zong-zi-yuan-lie-biao-shu-ju-ji-lun-wen-dai-ma-he-niu-ren-zhu-ye-deng>







## 十二、图像分类

图像分类实战：<https://mp.weixin.qq.com/s?__biz=MzI4ODY2NjYzMQ==&mid=2247486408&idx=2&sn=7749a2b9955c778ca1f692ff4764a776&chksm=ec3baea0db4c27b6314faaeff450b1000d3eb2ffb2656af202e06e165de952ad866b35e16db2&mpshare=1&scene=23&srcid=&sharer_sharetime=1575287124611&sharer_shareid=c16b66222c9e09eae50ae8f9081b0495#rd>









## 十三、 可视化

创建一个cnn模型

```
import torch
import torch.nn as nn
import torch.nn.functional as F
    

class Net(nn.Module):
    
    def __init__(self, weight):
        super(Net, self).__init__()
        # initializes the weights of the convolutional layer to be the weights of the 4 defined filters
        k_height, k_width = weight.shape[2:]
        # assumes there are 4 grayscale filters
        self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)
        self.conv.weight = torch.nn.Parameter(weight)
        # define a pooling layer
        self.pool = nn.MaxPool2d(2, 2)

    def forward(self, x):
        # calculates the output of a convolutional layer
        # pre- and post-activation
        conv_x = self.conv(x)
        activated_x = F.relu(conv_x)
        
        # applies pooling layer
        pooled_x = self.pool(activated_x)
        
        # returns all layers
        return conv_x, activated_x, pooled_x
```

加载权重以后获取其输出，进行可视化,每个filter都要可视化

```
def viz_layer(layer, n_filters= 4):
    fig = plt.figure(figsize=(20, 20))
    
    for i in range(n_filters):
        ax = fig.add_subplot(1, n_filters, i+1)
        ax.imshow(np.squeeze(layer[0,i].data.numpy()), cmap=plt.cm.hot_r)
        ax.set_title('Output %s' % str(i+1))
```

使用方法：

```
gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(0).unsqueeze(1)
conv_x, activated_layer, pooled_layer = model.forward(gray_img_tensor)

viz_layer(activated_layer)
viz_layer(pooled_layer)
```

